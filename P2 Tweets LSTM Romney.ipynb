{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Input\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import statistics\n",
    "from statistics import *\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Class</th>\n",
       "      <th>Your class label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Insidious!&lt;e&gt;Mitt Romney&lt;/e&gt;'s Bain Helped Phi...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.@WardBrenda @shortwave8669 @allanbourdius you...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;e&gt;Mitt Romney&lt;/e&gt; still doesn't &lt;a&gt;believe&lt;/a...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;e&gt;Romney&lt;/e&gt;'s &lt;a&gt;tax plan&lt;/a&gt; deserves a 2nd...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hope &lt;e&gt;Romney&lt;/e&gt; debate prepped w/ the same ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets Class  Your class label\n",
       "0  Insidious!<e>Mitt Romney</e>'s Bain Helped Phi...    -1               NaN\n",
       "2  .@WardBrenda @shortwave8669 @allanbourdius you...    -1               NaN\n",
       "3  <e>Mitt Romney</e> still doesn't <a>believe</a...    -1               NaN\n",
       "4  <e>Romney</e>'s <a>tax plan</a> deserves a 2nd...    -1               NaN\n",
       "5  Hope <e>Romney</e> debate prepped w/ the same ...     1               NaN"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'trainingObamaRomneytweets.xlsx'\n",
    "Raw_file = pd.ExcelFile(file)\n",
    "df_Romney = Raw_file.parse('Romney', skiprows = 1)\n",
    "df_Romney = Raw_file.parse('Romney', skiprows = 1)\n",
    "df_Romney = df_Romney[['1: positive, -1: negative, 0: neutral, 2: mixed', 'Class', 'Your class label']]\n",
    "df_Romney.rename(columns={'1: positive, -1: negative, 0: neutral, 2: mixed': 'Tweets'}, inplace=True)\n",
    "df_Romney.dropna(subset=['Tweets'], inplace=True)\n",
    "df_Romney.dropna(subset=['Class'], inplace=True)\n",
    "df_Romney = df_Romney[(df_Romney.Class == 0) | (df_Romney.Class == 1) | (df_Romney.Class == -1)]\n",
    "df_Romney.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Tweet_ID                                         Tweet_text\n",
      "0         1  <e>Romney</e> got 3 less minutes and had to de...\n",
      "1         2  <e>Mitt  </e>is beating him UP!  on his record...\n",
      "2         3  I actually like  <e>Romney </e>'s response to ...\n",
      "3         4  Just for that <a>immigration statement </a>tha...\n",
      "4         5  This man  <e>Romney  </e>is tearing this dude ...\n"
     ]
    }
   ],
   "source": [
    "df_test_Romney = pd.read_csv(\"Romney_Test_dataset_NO_Label.csv\", encoding = \"iso-8859-1\")\n",
    "print(df_test_Romney.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Tweets Class  Your class label\n",
      "0  insidiousmitt romneys bain helped philip morri...    -1               NaN\n",
      "2            you mean like romney cheated in primary    -1               NaN\n",
      "3  mitt romney still doesnt believe that we have ...    -1               NaN\n",
      "4  romneys tax plan deserves a nd look because he...    -1               NaN\n",
      "5  hope romney debate prepped w the same people a...     1               NaN\n",
      "   Tweet_ID                                         Tweet_text\n",
      "0         1  romney got  less minutes and had to debate can...\n",
      "1         2  mitt  is beating him up  on his record on cred...\n",
      "2         3  i actually like  romney s response to immigration\n",
      "3         4  just for that immigration statement that  romn...\n",
      "4         5  this man  romney  is tearing this dude up on e...\n"
     ]
    }
   ],
   "source": [
    "def preprocesstweets(tweet):\n",
    "    #if type(tweet) is str:\n",
    "    tweet = tweet.lower()\n",
    "    tweet = BeautifulSoup(tweet, \"html.parser\")\n",
    "    tweet = tweet.get_text()\n",
    "    tweet = re.sub(r\"http\\S+\", '', tweet)\n",
    "    tweet = re.sub(r'www.[^ ]+','', tweet)\n",
    "    tweet = re.sub(r'@[A-Za-z0-9]+','',tweet)\n",
    "    tweet = re.sub(r\"[^A-Za-z\\s]+\", '', tweet)\n",
    "    return tweet\n",
    "    \n",
    "\n",
    "df_Romney['Tweets'] = df_Romney['Tweets'].apply(preprocesstweets)\n",
    "df_test_Romney['Tweet_text'] = df_test_Romney[\"Tweet_text\"].apply(preprocesstweets)\n",
    "print(df_Romney.head())\n",
    "print(df_test_Romney.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Class</th>\n",
       "      <th>Your class label</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Tokens_nonames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>insidiousmitt romneys bain helped philip morri...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[insidiousmitt, romneys, bain, help, philip, m...</td>\n",
       "      <td>[insidiousmitt, bain, help, philip, morris, ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you mean like romney cheated in primary</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[mean, like, romney, cheat, primary]</td>\n",
       "      <td>[mean, like, cheat, primary]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mitt romney still doesnt believe that we have ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[mitt, romney, still, doesnt, believe, black, ...</td>\n",
       "      <td>[still, doesnt, believe, black, president]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>romneys tax plan deserves a nd look because he...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[romneys, tax, plan, deserve, nd, look, secret...</td>\n",
       "      <td>[tax, plan, deserve, nd, look, secret, one, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hope romney debate prepped w the same people a...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[hope, romney, debate, prepped, people, last, ...</td>\n",
       "      <td>[hope, debate, prepped, people, last, time]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets Class  Your class label  \\\n",
       "0  insidiousmitt romneys bain helped philip morri...    -1               NaN   \n",
       "2            you mean like romney cheated in primary    -1               NaN   \n",
       "3  mitt romney still doesnt believe that we have ...    -1               NaN   \n",
       "4  romneys tax plan deserves a nd look because he...    -1               NaN   \n",
       "5  hope romney debate prepped w the same people a...     1               NaN   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  [insidiousmitt, romneys, bain, help, philip, m...   \n",
       "2               [mean, like, romney, cheat, primary]   \n",
       "3  [mitt, romney, still, doesnt, believe, black, ...   \n",
       "4  [romneys, tax, plan, deserve, nd, look, secret...   \n",
       "5  [hope, romney, debate, prepped, people, last, ...   \n",
       "\n",
       "                                      Tokens_nonames  \n",
       "0  [insidiousmitt, bain, help, philip, morris, ge...  \n",
       "2                       [mean, like, cheat, primary]  \n",
       "3         [still, doesnt, believe, black, president]  \n",
       "4  [tax, plan, deserve, nd, look, secret, one, th...  \n",
       "5        [hope, debate, prepped, people, last, time]  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def create_tokens(tweet):\n",
    "    #if type(tweet) is str:\n",
    "    tweet = nltk.word_tokenize(tweet)\n",
    "    tweet_tokens = []\n",
    "    for i in tweet:\n",
    "        temp = lemmatizer.lemmatize(i, pos='v')\n",
    "        temp = lemmatizer.lemmatize(temp, pos='a')\n",
    "        if (i not in stopwords.words('english')) & (len(i) > 1):\n",
    "            tweet_tokens.append(temp)\n",
    "    return tweet_tokens\n",
    "\n",
    "def tokens_nonames(tweet):\n",
    "    name_words = ['mitt','romney','barack','obama','baracks','obamas','mitts','romneys']\n",
    "    tweet_tokens_nonames = []\n",
    "    for i in tweet:\n",
    "        if i not in name_words:\n",
    "            tweet_tokens_nonames.append(i)\n",
    "    return tweet_tokens_nonames\n",
    "                      \n",
    "df_Romney['Tokens'] = df_Romney['Tweets'].apply(create_tokens)\n",
    "df_Romney['Tokens_nonames'] = df_Romney['Tokens'].apply(tokens_nonames)\n",
    "\n",
    "df_test_Romney['Tokens'] = df_test_Romney['Tweet_text'].apply(create_tokens)\n",
    "df_test_Romney['Tokens_nonames'] = df_test_Romney['Tokens'].apply(tokens_nonames)\n",
    "df_Romney.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>Tweet_text</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Tokens_nonames</th>\n",
       "      <th>Processed Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>romney got  less minutes and had to debate can...</td>\n",
       "      <td>[romney, get, less, minutes, debate, candy, cr...</td>\n",
       "      <td>[get, less, minutes, debate, candy, crowley, s...</td>\n",
       "      <td>get less minutes debate candy crowley still pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>mitt  is beating him up  on his record on cred...</td>\n",
       "      <td>[mitt, beat, record, credibility, character]</td>\n",
       "      <td>[beat, record, credibility, character]</td>\n",
       "      <td>beat record credibility character</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>i actually like  romney s response to immigration</td>\n",
       "      <td>[actually, like, romney, response, immigration]</td>\n",
       "      <td>[actually, like, response, immigration]</td>\n",
       "      <td>actually like response immigration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>just for that immigration statement that  romn...</td>\n",
       "      <td>[immigration, statement, romney, answer, enoug...</td>\n",
       "      <td>[immigration, statement, answer, enough, get, ...</td>\n",
       "      <td>immigration statement answer enough get vote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>this man  romney  is tearing this dude up on e...</td>\n",
       "      <td>[man, romney, tear, dude, economics]</td>\n",
       "      <td>[man, tear, dude, economics]</td>\n",
       "      <td>man tear dude economics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet_ID                                         Tweet_text  \\\n",
       "0         1  romney got  less minutes and had to debate can...   \n",
       "1         2  mitt  is beating him up  on his record on cred...   \n",
       "2         3  i actually like  romney s response to immigration   \n",
       "3         4  just for that immigration statement that  romn...   \n",
       "4         5  this man  romney  is tearing this dude up on e...   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  [romney, get, less, minutes, debate, candy, cr...   \n",
       "1       [mitt, beat, record, credibility, character]   \n",
       "2    [actually, like, romney, response, immigration]   \n",
       "3  [immigration, statement, romney, answer, enoug...   \n",
       "4               [man, romney, tear, dude, economics]   \n",
       "\n",
       "                                      Tokens_nonames  \\\n",
       "0  [get, less, minutes, debate, candy, crowley, s...   \n",
       "1             [beat, record, credibility, character]   \n",
       "2            [actually, like, response, immigration]   \n",
       "3  [immigration, statement, answer, enough, get, ...   \n",
       "4                       [man, tear, dude, economics]   \n",
       "\n",
       "                                    Processed Tweets  \n",
       "0  get less minutes debate candy crowley still pe...  \n",
       "1                  beat record credibility character  \n",
       "2                 actually like response immigration  \n",
       "3       immigration statement answer enough get vote  \n",
       "4                            man tear dude economics  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def detoken(tweet):\n",
    "    detkn = ' '.join([i for i in tweet])\n",
    "    return detkn\n",
    "                      \n",
    "df_Romney['Processed Tweets'] = df_Romney['Tokens_nonames'].apply(detoken)\n",
    "\n",
    "df_test_Romney['Processed Tweets'] = df_test_Romney['Tokens_nonames'].apply(detoken)\n",
    "df_Romney.head()\n",
    "df_test_Romney.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "bow_romney = tokenizer.fit_on_texts(df_Romney['Processed Tweets'].values)\n",
    "bow_romney = tokenizer.texts_to_sequences(df_Romney['Processed Tweets'].values)\n",
    "\n",
    "\n",
    "bow_test_romney = tokenizer.fit_on_texts(df_test_Romney['Processed Tweets'].values)\n",
    "bow_test_romney = tokenizer.texts_to_sequences(df_test_Romney['Processed Tweets'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1900, 300)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_romney = pad_sequences(bow_romney, maxlen=300)\n",
    "pad_test_romney = pad_sequences(bow_test_romney, maxlen=300)\n",
    "pad_test_romney.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_romney\n",
    "Y_train = df_Romney['Class']\n",
    "Y_train_NN = pd.get_dummies(Y_train).values\n",
    "X_test = pad_test_romney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "f = open(\"glove.6B.50d.txt\", encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded word vectors. 400000\n"
     ]
    }
   ],
   "source": [
    "print('Loaded word vectors.',len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8661\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)\n",
    "embedding_matrix = np.zeros((vocab_size, 50))\n",
    "#print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8661, 50)\n"
     ]
    }
   ],
   "source": [
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 300, 50)           433050    \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 300, 50)           20200     \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 300, 50)           20200     \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 30)                9720      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 93        \n",
      "=================================================================\n",
      "Total params: 483,263\n",
      "Trainable params: 483,263\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, weights=[embedding_matrix], input_length=300, trainable=True))\n",
    "model.add(LSTM(50, return_sequences=True))\n",
    "model.add(LSTM(50, return_sequences=True))\n",
    "model.add(LSTM(30, return_sequences=False))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "5648/5648 [==============================] - 34s 6ms/step - loss: 1.0163 - acc: 0.5080\n",
      "Epoch 2/3\n",
      "5648/5648 [==============================] - 29s 5ms/step - loss: 0.9793 - acc: 0.5191\n",
      "Epoch 3/3\n",
      "5648/5648 [==============================] - 31s 5ms/step - loss: 0.9096 - acc: 0.5652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28a6442e438>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train_NN, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM = load_model('model_LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_LSTM = model_LSTM.predict(X_test)\n",
    "ans = np.argmax(pred_LSTM, axis=1)\n",
    "\n",
    "Y_LSTM= []\n",
    "for i in ans:\n",
    "    if i == 0:\n",
    "        Y_LSTM.append(-1)\n",
    "    if i == 1:\n",
    "        Y_LSTM.append(0)\n",
    "    if i == 2:\n",
    "        Y_LSTM.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_LSTM = load_model('model_LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, -1, -1, -1, -1, -1, 0, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, 1, -1, 1, -1, 1, 0, -1, -1, -1, -1, -1, -1, 1, -1, 0, 0, -1, 1, -1, -1, -1, -1, 1, 1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, 1, 1, 1, -1, -1, -1, -1, 1, -1, -1, -1, 1, -1, -1, 1, -1, -1, -1, 1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 1, -1, 1, -1, 1, 1, -1, 0, -1, 1, 0, -1, -1, -1, 0, -1, 0, 0, 1, -1, -1, -1, -1, 1, 0, 0, -1, -1, -1, -1, 0, 0, -1, 1, 1, 1, 0, -1, 1, -1, 0, -1, 1, -1, -1, -1, 1, 0, -1, -1, 1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 1, 0, -1, 1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, 0, 1, -1, -1, -1, 1, -1, 0, 1, -1, 1, -1, -1, 0, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, 1, -1, 0, 1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, 1, 0, 1, 1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 0, 1, -1, -1, -1, 1, -1, 1, -1, 0, 0, -1, -1, -1, 1, -1, 0, -1, -1, 0, 1, -1, -1, 1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, -1, 0, -1, 1, -1, -1, -1, -1, 1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, 1, 0, -1, 1, 1, -1, -1, -1, -1, -1, -1, 1, 1, 0, -1, -1, 1, 1, -1, -1, 0, 1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, 1, -1, 1, 0, -1, -1, -1, -1, -1, 1, -1, 1, -1, 1, -1, 0, 1, -1, -1, -1, -1, 0, 0, 1, 1, 1, 1, 0, -1, -1, 1, -1, -1, 1, 0, 1, -1, -1, -1, -1, -1, -1, 1, 1, -1, -1, -1, 0, 1, -1, -1, 1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 1, -1, 1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 0, -1, -1, -1, -1, 1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 1, 0, 1, -1, 0, -1, -1, -1, 1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, 1, -1, -1, -1, 0, -1, -1, -1, -1, 1, 0, 1, -1, 1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 1, 0, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, 1, -1, 1, 1, 0, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, -1, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, 1, -1, -1, 0, -1, 0, -1, 1, 0, 1, -1, 1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 0, -1, -1, 0, -1, 1, -1, 1, -1, -1, -1, -1, 0, 1, -1, 1, -1, -1, 1, 1, -1, -1, 1, 0, -1, 1, -1, -1, 1, 0, -1, -1, 0, -1, -1, 1, 1, -1, -1, 1, -1, 0, 0, -1, -1, 0, 1, -1, 0, -1, -1, 1, -1, -1, -1, -1, -1, -1, 0, -1, 1, -1, -1, 1, -1, 1, -1, 0, 1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 0, -1, 1, 1, 1, -1, 0, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 0, -1, -1, -1, -1, 1, -1, -1, 1, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 0, -1, 1, -1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 1, -1, 0, -1, 0, 1, -1, 0, 1, 0, -1, 0, -1, -1, -1, -1, 1, -1, 1, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 1, 1, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, 1, 0, 0, -1, -1, -1, -1, 0, -1, -1, 0, 1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 1, 0, -1, 1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 0, 1, 1, 1, -1, -1, 1, -1, 0, 1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, -1, 1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, 1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, 1, 1, -1, -1, -1, -1, 1, -1, 1, 1, 0, -1, -1, -1, -1, 0, -1, 0, 1, -1, -1, -1, -1, 1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 1, -1, -1, -1, 1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, 0, 1, -1, -1, -1, 1, -1, -1, -1, 0, -1, 1, 1, -1, 1, 0, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 1, 0, -1, -1, 1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 1, -1, -1, -1, 0, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, 1, -1, 1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 1, 1, -1, 0, -1, 1, 1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, 0, 1, -1, 1, -1, -1, 1, 1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 0, 1, 1, 0, 1, 0, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 1, -1, -1, 1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 0, -1, 0, -1, 0, 0, 0, 1, -1, -1, -1, -1, -1, 1, -1, 0, -1, 1, -1, 0, 1, -1, 0, -1, -1, -1, -1, 1, 0, -1, 1, -1, -1, 1, 0, 1, -1, 1, -1, -1, -1, -1, 0, 1, -1, -1, 1, -1, 1, -1, 0, 1, -1, 1, -1, -1, -1, 0, 0, 0, -1, 1, -1, -1, -1, 1, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 0, -1, -1, 1, 0, -1, -1, -1, -1, -1, 0, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 0, -1, -1, -1, 1, -1, 1, 1, -1, 1, 1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 1, 1, -1, -1, -1, 0, -1, -1, 0, 1, -1, -1, 0, -1, -1, 1, -1, -1, 1, 1, -1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 1, 1, -1, 1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 1, -1, 1, -1, 1, -1, -1, 1, 1, -1, -1, 0, 1, -1, -1, -1, 1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 1, 0, -1, 1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 1, 1, 0, -1, -1, -1, 1, -1, -1, -1, -1, 1, 1, -1, -1, 0, 0, 0, 1, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, 1, -1, 0, 0, 0, -1, -1, -1, 0, 1, 1, -1, -1, -1, 1, -1, -1, -1, -1, 1, -1, 1, -1, -1, -1, -1, -1, -1, 1, 0, -1, -1, 1, 0, 1, -1, -1, -1, 0, -1, -1, -1, 0, 1, 0, 0, 1, -1, -1, -1, -1, 1, -1, 1, 0, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 0, -1, 1, 1, 1, -1, -1, -1, 1, -1, 1, -1, -1, 1, -1, 1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 0, -1, 1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, -1, 0, -1, 1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 1, -1, -1, 0, 1, -1, -1, 1, 1, 0, 0, -1, 1, -1, 1, -1, 1, -1, -1, -1, 1, 0, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, 1, -1, -1, -1, -1, -1, -1, 1, -1, 1, 0, -1, -1, -1, 1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 1, 0, -1, -1, -1, -1, -1, 1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 1, 0, -1, 1, 1, 0, -1, -1, -1, -1, 1, -1, -1, 0, -1, 1, -1, -1, -1, -1, -1, 1, 1, -1, -1, 0, 1, 1, -1, -1, 1, 1, -1, -1, -1, -1, -1, 0, 1, -1, -1, -1, 1, -1, -1, -1, 1, -1, -1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(Y_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "Output = \"praveen_chandrasekaran_saikrishnan_thiruvarpuneelakantan_Romney.txt\"\n",
    "\n",
    "def Filetxt(fname, output):\n",
    "    f=open(fname, \"w+\")\n",
    "    for i in range(len(output)):\n",
    "        f.write(str(df_test_Romney.loc[i,\"Tweet_ID\"]) + \";;\" + str(output[i]) + \"\\n\")\n",
    "\n",
    "Filetxt(Output,Y_LSTM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
