{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Input\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import statistics\n",
    "from statistics import *\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Class</th>\n",
       "      <th>Your class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kirkpatrick, who wore a baseball cap embroider...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#&lt;e&gt;obama&lt;/e&gt; debates that Cracker Ass Cracker...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Hollivan @hereistheanswer  Youre missing the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I was raised as a Democrat  left the party yea...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The &lt;e&gt;Obama camp&lt;/e&gt; can't afford to lower ex...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets Class  Your class\n",
       "0  Kirkpatrick, who wore a baseball cap embroider...     0         NaN\n",
       "2  #<e>obama</e> debates that Cracker Ass Cracker...     1         NaN\n",
       "4  @Hollivan @hereistheanswer  Youre missing the ...     0         NaN\n",
       "6  I was raised as a Democrat  left the party yea...    -1         NaN\n",
       "7  The <e>Obama camp</e> can't afford to lower ex...     0         NaN"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'trainingObamaRomneytweets.xlsx'\n",
    "Raw_file = pd.ExcelFile(file)\n",
    "df_Obama = Raw_file.parse('Obama',skiprows = 1)\n",
    "df_Obama = df_Obama[['1: positive, -1: negative, 0: neutral, 2: mixed', 'Class', 'Your class']]\n",
    "df_Obama.rename(columns={'1: positive, -1: negative, 0: neutral, 2: mixed': 'Tweets'}, inplace=True)\n",
    "df_Obama.dropna(subset=['Tweets'], inplace=True)\n",
    "df_Obama.dropna(subset=['Class'], inplace=True)\n",
    "df_Obama = df_Obama[(df_Obama.Class == 0) | (df_Obama.Class == 1) | (df_Obama.Class == -1)]\n",
    "df_Obama.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Tweet_ID                                         Tweet_text\n",
      "0         1  <e>Obama</e> has to maintain his professionali...\n",
      "1         2  <e>Obama</e> went into the debate swinging and...\n",
      "2         3  Ditto. I started @247LS 4 years ago. RT @bmorr...\n",
      "3         4  I absolutely love <e>Obama</e>'s view in <a>im...\n",
      "4         5  I'm agreeing completely with <e>Obama</e>'s st...\n"
     ]
    }
   ],
   "source": [
    "df_test_Obama = pd.read_csv(\"Obama_Test_dataset_NO_Label.csv\", encoding = \"iso-8859-1\")\n",
    "print(df_test_Obama.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Praveen\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "#df_Obama = pd.concat([df_Obama, df_Romney])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Class</th>\n",
       "      <th>Your class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kirkpatrick who wore a baseball cap embroidere...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obama debates that cracker ass cracker tonight...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>youre missing the point  im afraid you do n...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i was raised as a democrat  left the party yea...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the obama camp cant afford to lower expectatio...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets Class  Your class\n",
       "0  kirkpatrick who wore a baseball cap embroidere...     0         NaN\n",
       "2  obama debates that cracker ass cracker tonight...     1         NaN\n",
       "4     youre missing the point  im afraid you do n...     0         NaN\n",
       "6  i was raised as a democrat  left the party yea...    -1         NaN\n",
       "7  the obama camp cant afford to lower expectatio...     0         NaN"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocesstweets(tweet):\n",
    "    #if type(tweet) is str:\n",
    "    tweet = tweet.lower()\n",
    "    tweet = BeautifulSoup(tweet, \"html.parser\")\n",
    "    tweet = tweet.get_text()\n",
    "    tweet = re.sub(r\"http\\S+\", '', tweet)\n",
    "    tweet = re.sub(r'www.[^ ]+','', tweet)\n",
    "    tweet = re.sub(r'@[A-Za-z0-9]+','',tweet)\n",
    "    tweet = re.sub(r\"[^A-Za-z\\s]+\", '', tweet)\n",
    "    return tweet\n",
    "    \n",
    "\n",
    "df_Obama['Tweets'] = df_Obama['Tweets'].apply(preprocesstweets)\n",
    "df_test_Obama['Tweet_text'] = df_test_Obama['Tweet_text'].apply(preprocesstweets)\n",
    "df_Obama.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Class</th>\n",
       "      <th>Your class</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Tokens_nonames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kirkpatrick who wore a baseball cap embroidere...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[kirkpatrick, wear, baseball, cap, embroider, ...</td>\n",
       "      <td>[kirkpatrick, wear, baseball, cap, embroider, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obama debates that cracker ass cracker tonight...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[obama, debate, cracker, ass, cracker, tonight...</td>\n",
       "      <td>[debate, cracker, ass, cracker, tonight, tune,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>youre missing the point  im afraid you do n...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[youre, miss, point, im, afraid, understand, b...</td>\n",
       "      <td>[youre, miss, point, im, afraid, understand, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i was raised as a democrat  left the party yea...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[raise, democrat, leave, party, years, ago, li...</td>\n",
       "      <td>[raise, democrat, leave, party, years, ago, li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the obama camp cant afford to lower expectatio...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[obama, camp, cant, afford, low, expectations,...</td>\n",
       "      <td>[camp, cant, afford, low, expectations, tonigh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets Class  Your class  \\\n",
       "0  kirkpatrick who wore a baseball cap embroidere...     0         NaN   \n",
       "2  obama debates that cracker ass cracker tonight...     1         NaN   \n",
       "4     youre missing the point  im afraid you do n...     0         NaN   \n",
       "6  i was raised as a democrat  left the party yea...    -1         NaN   \n",
       "7  the obama camp cant afford to lower expectatio...     0         NaN   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  [kirkpatrick, wear, baseball, cap, embroider, ...   \n",
       "2  [obama, debate, cracker, ass, cracker, tonight...   \n",
       "4  [youre, miss, point, im, afraid, understand, b...   \n",
       "6  [raise, democrat, leave, party, years, ago, li...   \n",
       "7  [obama, camp, cant, afford, low, expectations,...   \n",
       "\n",
       "                                      Tokens_nonames  \n",
       "0  [kirkpatrick, wear, baseball, cap, embroider, ...  \n",
       "2  [debate, cracker, ass, cracker, tonight, tune,...  \n",
       "4  [youre, miss, point, im, afraid, understand, b...  \n",
       "6  [raise, democrat, leave, party, years, ago, li...  \n",
       "7  [camp, cant, afford, low, expectations, tonigh...  "
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def create_tokens(tweet):\n",
    "    #if type(tweet) is str:\n",
    "    tweet = nltk.word_tokenize(tweet)\n",
    "    tweet_tokens = []\n",
    "    for i in tweet:\n",
    "        temp = lemmatizer.lemmatize(i, pos='v')\n",
    "        temp = lemmatizer.lemmatize(temp, pos='a')\n",
    "        if (i not in stopwords.words('english')) & (len(i) > 1):\n",
    "            tweet_tokens.append(temp)\n",
    "    return tweet_tokens\n",
    "\n",
    "def tokens_nonames(tweet):\n",
    "    name_words = ['mitt','romney','barack','obama','baracks','obamas','mitts','romneys']\n",
    "    tweet_tokens_nonames = []\n",
    "    for i in tweet:\n",
    "        if i not in name_words:\n",
    "            tweet_tokens_nonames.append(i)\n",
    "    return tweet_tokens_nonames\n",
    "                      \n",
    "df_Obama['Tokens'] = df_Obama['Tweets'].apply(create_tokens)\n",
    "df_Obama['Tokens_nonames'] = df_Obama['Tokens'].apply(tokens_nonames)\n",
    "df_test_Obama['Tokens'] = df_test_Obama['Tweet_text'].apply(create_tokens)\n",
    "df_test_Obama['Tokens_nonames'] = df_test_Obama['Tokens'].apply(tokens_nonames)\n",
    "df_Obama.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Class</th>\n",
       "      <th>Your class</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Tokens_nonames</th>\n",
       "      <th>Processed Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kirkpatrick who wore a baseball cap embroidere...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[kirkpatrick, wear, baseball, cap, embroider, ...</td>\n",
       "      <td>[kirkpatrick, wear, baseball, cap, embroider, ...</td>\n",
       "      <td>kirkpatrick wear baseball cap embroider signat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obama debates that cracker ass cracker tonight...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[obama, debate, cracker, ass, cracker, tonight...</td>\n",
       "      <td>[debate, cracker, ass, cracker, tonight, tune,...</td>\n",
       "      <td>debate cracker ass cracker tonight tune teamobama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>youre missing the point  im afraid you do n...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[youre, miss, point, im, afraid, understand, b...</td>\n",
       "      <td>[youre, miss, point, im, afraid, understand, b...</td>\n",
       "      <td>youre miss point im afraid understand big pict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i was raised as a democrat  left the party yea...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[raise, democrat, leave, party, years, ago, li...</td>\n",
       "      <td>[raise, democrat, leave, party, years, ago, li...</td>\n",
       "      <td>raise democrat leave party years ago lifetime ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the obama camp cant afford to lower expectatio...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[obama, camp, cant, afford, low, expectations,...</td>\n",
       "      <td>[camp, cant, afford, low, expectations, tonigh...</td>\n",
       "      <td>camp cant afford low expectations tonights deb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets Class  Your class  \\\n",
       "0  kirkpatrick who wore a baseball cap embroidere...     0         NaN   \n",
       "2  obama debates that cracker ass cracker tonight...     1         NaN   \n",
       "4     youre missing the point  im afraid you do n...     0         NaN   \n",
       "6  i was raised as a democrat  left the party yea...    -1         NaN   \n",
       "7  the obama camp cant afford to lower expectatio...     0         NaN   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  [kirkpatrick, wear, baseball, cap, embroider, ...   \n",
       "2  [obama, debate, cracker, ass, cracker, tonight...   \n",
       "4  [youre, miss, point, im, afraid, understand, b...   \n",
       "6  [raise, democrat, leave, party, years, ago, li...   \n",
       "7  [obama, camp, cant, afford, low, expectations,...   \n",
       "\n",
       "                                      Tokens_nonames  \\\n",
       "0  [kirkpatrick, wear, baseball, cap, embroider, ...   \n",
       "2  [debate, cracker, ass, cracker, tonight, tune,...   \n",
       "4  [youre, miss, point, im, afraid, understand, b...   \n",
       "6  [raise, democrat, leave, party, years, ago, li...   \n",
       "7  [camp, cant, afford, low, expectations, tonigh...   \n",
       "\n",
       "                                    Processed Tweets  \n",
       "0  kirkpatrick wear baseball cap embroider signat...  \n",
       "2  debate cracker ass cracker tonight tune teamobama  \n",
       "4  youre miss point im afraid understand big pict...  \n",
       "6  raise democrat leave party years ago lifetime ...  \n",
       "7  camp cant afford low expectations tonights deb...  "
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def detoken(tweet):\n",
    "    detkn = ' '.join([i for i in tweet])\n",
    "    return detkn\n",
    "                      \n",
    "df_Obama['Processed Tweets'] = df_Obama['Tokens_nonames'].apply(detoken)\n",
    "df_test_Obama['Processed Tweets'] = df_test_Obama['Tokens_nonames'].apply(detoken)\n",
    "df_Obama.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "bow_obama = tokenizer.fit_on_texts(df_Obama['Processed Tweets'].values)\n",
    "bow_obama = tokenizer.texts_to_sequences(df_Obama['Processed Tweets'].values)\n",
    "\n",
    "bow_test_obama = tokenizer.fit_on_texts(df_test_Obama['Processed Tweets'].values)\n",
    "bow_test_obama = tokenizer.texts_to_sequences(df_test_Obama['Processed Tweets'].values)\n",
    "#print(tokenizer.word_index)\n",
    "#bow_obama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5470, 300)"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_obama = pad_sequences(bow_obama, maxlen=300)\n",
    "pad_test_obama = pad_sequences(bow_test_obama, maxlen=300)\n",
    "pad_obama.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_obama\n",
    "Y_train = df_Obama['Class']\n",
    "X_test = pad_test_obama\n",
    "Y_train_NN = pd.get_dummies(Y_train).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "f = open(\"glove.6B.50d.txt\", encoding=\"utf8\")\n",
    "for line in f:\n",
    "    #print(line)\n",
    "    values = line.split()\n",
    "    #print(values)\n",
    "    word = values[0]\n",
    "    #print(word)\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    #print(coefs)\n",
    "    embeddings_index[word] = coefs\n",
    "    #print(embeddings_index)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded word vectors. 400000\n"
     ]
    }
   ],
   "source": [
    "print('Loaded word vectors.',len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "#print(vocab_size)\n",
    "embedding_matrix = np.zeros((vocab_size, 50))\n",
    "#print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    #print(word)\n",
    "    #print(embedding_vector)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#print(vocab_size)\n",
    "#print(embedding_matrix[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Embedding(vocab_size, 50, weights=[embedding_matrix], input_length=300, trainable=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(LSTM(30, return_sequences=True))\n",
    "model.add(LSTM(30, return_sequences=True))\n",
    "model.add(LSTM(20, return_sequences=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_48 (Embedding)     (None, 300, 50)           454950    \n",
      "_________________________________________________________________\n",
      "lstm_43 (LSTM)               (None, 300, 30)           9720      \n",
      "_________________________________________________________________\n",
      "lstm_44 (LSTM)               (None, 300, 30)           7320      \n",
      "_________________________________________________________________\n",
      "lstm_45 (LSTM)               (None, 20)                4080      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 476,133\n",
      "Trainable params: 476,133\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4923 samples, validate on 547 samples\n",
      "Epoch 1/4\n",
      " 128/4923 [..............................] - ETA: 1:19 - loss: 0.6669 - acc: 0.7500"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-484-65fc56cdde57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train_NN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    185\u001b[0m                             ins[:-1], batch_ids) + [ins[-1]]\n\u001b[0;32m    186\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m                         \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m                     raise TypeError('TypeError while preparing batch. '\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[1;34m(arrays, start, stop)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train_NN, epochs=4, batch_size=64, validation_split= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_LSTM_Obama.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM = load_model('model_LSTM_Obama.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_LSTM = model_LSTM.predict(X_test)\n",
    "ans = np.argmax(pred_LSTM, axis=1)\n",
    "\n",
    "Y_LSTM= []\n",
    "for i in ans:\n",
    "    if i == 0:\n",
    "        Y_LSTM.append(-1)\n",
    "    if i == 1:\n",
    "        Y_LSTM.append(0)\n",
    "    if i == 2:\n",
    "        Y_LSTM.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, -1, -1, -1, 0, 1, 0, 0, -1, 0, 1, 1, 0, -1, -1, 0, -1, 1, 0, 0, 0, 0, -1, 1, 1, 1, -1, 1, 0, 1, 0, -1, -1, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 1, -1, 0, -1, -1, 0, 0, 0, 1, 0, -1, -1, 0, 1, -1, 1, 1, 1, 0, 0, 1, 1, -1, -1, 1, -1, 0, -1, 1, 1, 1, 0, -1, 1, 0, 1, 1, 0, 1, 0, -1, 0, 0, 0, 0, 1, 0, 0, 0, -1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, -1, 0, 0, -1, 0, -1, -1, -1, 1, -1, -1, 1, 0, -1, -1, 0, 0, 0, 1, 1, 1, 0, -1, 0, 1, 0, 1, 1, 0, 0, -1, 1, 1, 1, -1, 1, 0, 1, 0, 1, 0, 0, -1, 1, 1, -1, 0, 1, 1, 1, 1, 0, -1, 0, 1, 0, 0, 1, 0, 0, 1, -1, 0, 1, 1, 0, 0, -1, -1, 1, 1, -1, 1, -1, 0, 0, -1, 1, 0, -1, 1, 1, -1, 0, -1, 1, 0, 1, 0, 1, -1, 0, 1, -1, -1, 1, 1, 0, -1, 1, -1, -1, 0, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, 1, -1, -1, 1, 0, 1, 0, -1, 0, 1, 0, 1, 1, 1, -1, 1, 0, 0, -1, 0, 0, -1, 0, -1, -1, 1, 0, 1, 0, 1, 0, -1, 0, 1, 0, 1, 1, 1, -1, 0, -1, 0, 1, 0, 0, 1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 0, 0, 1, 1, 0, -1, 0, 0, -1, -1, 0, 1, -1, 0, 0, 1, 1, 1, 0, -1, 1, 1, 1, 0, 1, 1, 1, -1, 1, 0, -1, 1, 1, 0, 1, -1, 0, 0, -1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, -1, 0, 1, 1, 0, 1, -1, -1, 1, -1, 1, 1, 1, 0, 0, -1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, -1, 0, 1, 0, 0, 1, 1, 1, 1, -1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, -1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, -1, 1, 0, -1, -1, -1, -1, 1, 1, -1, 0, -1, -1, 0, 0, 0, 0, 1, -1, 0, 1, 1, 0, 1, -1, -1, -1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, -1, -1, 1, -1, 0, -1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, -1, -1, 1, 0, 1, 1, 1, 0, 1, 1, 1, -1, -1, 1, 1, 0, 1, 1, 1, 1, 1, -1, 1, -1, -1, 1, -1, 1, -1, 0, 1, 1, 0, 1, 0, -1, 1, 0, -1, -1, 1, 1, 0, 0, 1, 0, 1, 0, 1, -1, 1, 0, 0, 1, -1, -1, 1, 1, -1, 0, 1, 1, 0, 0, -1, 0, 1, -1, 0, -1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, -1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, -1, 1, 0, 1, 1, 0, 1, 0, 1, 1, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 1, -1, 1, 1, 0, 1, 1, 0, 1, -1, 0, 1, 0, 0, 0, 0, 1, -1, -1, 1, 1, 1, 0, 0, -1, 1, -1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, -1, -1, -1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 1, 0, -1, -1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, -1, 1, -1, 0, -1, 0, 1, -1, 0, -1, 0, 0, 0, 1, -1, -1, 0, 1, 0, -1, -1, 1, -1, 0, 0, 1, -1, 0, 1, 0, 0, 1, -1, 0, 0, -1, -1, 0, 0, -1, 0, 1, -1, 0, 0, -1, -1, -1, -1, 1, 0, 0, -1, 0, -1, -1, 0, 1, 0, -1, -1, -1, 0, 0, 0, 1, -1, 0, 0, -1, 0, -1, 0, 1, 0, 1, 0, 1, 0, 0, -1, 0, 1, -1, -1, 0, 1, 1, 1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, 1, 0, 1, 1, -1, -1, -1, 1, -1, 0, -1, -1, 1, 0, -1, -1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, -1, -1, 1, 1, -1, 1, -1, -1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, -1, 0, 1, 1, -1, -1, 1, 0, 1, 0, -1, 1, 0, 1, 0, -1, 0, 1, 1, 0, 1, -1, 0, 1, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 1, 1, 1, 0, 1, -1, 0, -1, 1, -1, 0, 0, -1, 1, 0, -1, 0, 1, 1, 0, 1, -1, 1, 1, -1, -1, 0, 1, -1, -1, 1, 0, 1, 0, 0, 1, -1, 1, -1, 0, -1, 1, 0, 1, -1, 1, 0, 1, 1, -1, 0, -1, 0, 0, 1, 1, -1, -1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, -1, 0, 1, 0, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, -1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, -1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, -1, -1, 1, 0, 0, -1, -1, 0, 0, 1, -1, 0, 0, 1, 0, 0, -1, 1, 1, 0, 0, 0, 0, 1, -1, 1, 0, 0, 1, 0, 0, -1, 0, -1, 1, 1, 1, 0, 1, 1, 0, 0, -1, -1, 0, -1, 1, -1, 0, 0, 0, 0, -1, 0, 1, 1, 1, -1, -1, 0, 1, 1, 0, 0, -1, 1, -1, 0, -1, 0, 1, -1, -1, 0, -1, 0, 0, 1, 0, -1, 1, 0, 0, 1, 0, 0, -1, 0, -1, 1, 1, -1, 1, -1, -1, -1, 1, 0, -1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, -1, 0, -1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, -1, -1, -1, -1, 1, 1, 0, 0, -1, 0, 0, 1, -1, 0, -1, 1, 1, 1, 0, 1, 1, -1, -1, 1, -1, 1, 0, 0, 1, -1, 1, -1, 1, 0, 0, 1, -1, 1, 1, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 1, 0, -1, 0, -1, 1, 0, 0, 0, -1, 1, -1, 0, -1, 1, -1, -1, -1, 0, 0, 1, -1, 1, 0, 0, 0, 1, 0, 0, -1, 1, -1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, -1, 1, 0, 0, -1, 0, 0, 0, 0, 1, 0, -1, 1, -1, -1, -1, -1, 0, 0, -1, 0, 0, 1, 1, 0, 1, 0, 0, -1, -1, -1, 1, 0, 1, -1, -1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, -1, -1, 0, 0, -1, 1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 1, 0, -1, -1, -1, -1, 0, 1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, -1, -1, 0, 0, 1, 1, 1, -1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, -1, 0, -1, 0, -1, 0, 1, -1, 1, -1, -1, -1, 0, 0, 0, 1, -1, 0, 1, -1, 0, -1, 1, -1, 0, 0, 1, 1, 0, -1, 0, -1, 0, 1, -1, -1, 0, 0, -1, 1, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, 1, -1, -1, -1, -1, 0, -1, 1, -1, 0, -1, 0, 0, 0, 0, 1, -1, -1, -1, 1, 0, 0, -1, -1, 0, 0, 0, 0, 1, -1, -1, 0, 0, 0, -1, -1, 1, -1, 1, 0, 1, 1, -1, 0, -1, -1, -1, 1, -1, 1, -1, 1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 1, -1, 0, 1, 0, 1, -1, 0, 1, 0, -1, 1, -1, 0, 0, 0, 0, 1, 1, -1, 0, -1, 0, 1, -1, 0, 0, 0, 0, 0, 0, 1, -1, -1, -1, -1, 0, 1, 1, -1, 0, -1, 0, 0, 1, -1, -1, 0, -1, 1, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, -1, 1, -1, -1, 0, -1, 0, 0, -1, 0, 0, 1, -1, 0, 0, -1, -1, 0, -1, 0, 0, 1, 0, -1, 1, -1, -1, 0, -1, 1, 0, 0, 1, -1, 1, 0, 0, 1, -1, 0, 0, 0, -1, -1, 0, 0, 1, -1, -1, 0, 1, 0, 0, 1, 1, -1, -1, -1, 1, 1, -1, 1, 0, 1, -1, -1, -1, -1, 0, 0, 0, -1, 1, -1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, -1, 1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 1, 0, -1, 1, 0, 1, 0, 0, -1, 0, 0, -1, 1, 1, 1, -1, 0, 0, -1, 0, 0, 0, -1, 1, -1, 1, 1, 1, -1, 1, -1, -1, -1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, -1, 0, 0, 0, 0, 1, -1, 1, -1, 0, 0, -1, 0, 1, 1, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 1, 0, 0, 0, -1, 1, -1, 0, 0, 1, 1, -1, 0, 0, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1, 1, 0, 0, -1, 1, 0, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, 1, 0, 0, 0, -1, -1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, -1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, -1, 1, -1, -1, 0, 1, -1, 1, -1, -1, -1, 0, 0, 1, 1, 0, 0, 0, -1, 0, -1, 0, 1, 1, -1, 0, -1, 1, 0, 1, -1, -1, 1, 1, -1, 0, 1, 1, -1, -1, 0, -1, 0, -1, -1, 0, 1, -1, -1, -1, 0, 1, -1, -1, -1, -1, 0, -1, 1, -1, -1, 0, 0, -1, -1, 1, -1, -1, 0, -1, 0, 1, 0, 1, 0, 0, 1, 0, -1, 1, 1, 1, 0, -1, -1, -1, 0, 0, 0, -1, 1, 0, 0, 0, 1, 1, 1, 1, -1, 0, 0, 0, -1, -1, -1, 0, 1, -1, -1, 1, 0, 1, 0, 1, -1, 1, 0, -1, -1, 0, 0, 0, 1, 0, 0, -1, 0, 0, 1, 0, 0, 1, 0, 0, -1, 0, 0, 0, -1, -1, 1, 1, 0, 1, 0, 1, 1, -1, -1, 1, 1, -1, 1, -1, 1, -1, -1, 0, -1, -1, -1, 0, 1, 0, 1, 0, -1, 0, -1, 0, -1, 1, 0, 1, -1, 1, -1, 0, 0, 1, 0, 1, 0, -1, 1, -1, 1, -1, 0, 0, 0, 0, -1, -1, 1, 0, 1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 1, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(Y_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "Output = \"praveen_chandrasekaran_saikrishnan_thiruvarpuneelakantan_Obama.txt\"\n",
    "\n",
    "def Filetxt(fname, output):\n",
    "    f=open(fname, \"w+\")\n",
    "    for i in range(len(output)):\n",
    "        f.write(str(df_test_Obama.loc[i,\"Tweet_ID\"]) + \";;\" + str(output[i]) + \"\\n\")\n",
    "\n",
    "Filetxt(Output,Y_LSTM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
